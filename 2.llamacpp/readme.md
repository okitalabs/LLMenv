# LLMå®Ÿè¡Œç’°å¢ƒã®æ§‹ç¯‰
# llama-cpp-python
llama-cpp-pythonã¯ã€é‡å­åŒ–GGUFå½¢å¼ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ llama-cppã‚’Pythonã‹ã‚‰ä½¿ãˆã‚‹ã‚ˆã†ã«ã—ãŸãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€‚  
åŒæ¢±ã—ã¦ã„ã‚‹`llama_cpp.server`ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€OpenAI APIäº’æ›ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦ã€å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹ï¼ˆllama-cppã®HTTP Serverã¨ã¯åˆ¥ç‰©ï¼‰ã€‚  
è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¦ãŠãã¨ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†æ™‚ã«modelåã«ã‚ˆã£ã¦ã€å®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹ã€‚


## æ§‹æˆæƒ…å ±
### LLMãƒ¢ãƒ‡ãƒ«
|modelå|é‡å­åŒ–|å¯¾è±¡ãƒ¢ãƒ‡ãƒ«|
|:----|:----|:----|
|vicuna-13b|q8_0|[TheBloke/vicuna-13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGUF) |
|karakuri-8x7b|q6_K|[mmnga/karakuri-lm-8x7b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf)|


### Dockerè¨­å®š
|Dockerå|Host Port|Docker Port|Host Dir|Docker Dir|
|:----|:----|:----|:----|:----|
|llamacpp|20080|20080|/home/llm/llamacpp|/llamacpp|


<br>
<hr>


# æ§‹ç¯‰æ‰‹é †
## LLMãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

llama-cpp-pythonç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«(GGUF)ã‚’`$HOME/llamacpp/model/`ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãã€‚

```bash
$ mkdir $HOME/llamacpp/model ## ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
$ cd $HOME/llamacpp/model

## vicuna-13b-v1.5.Q8_0.gguf
$ wget https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGUF/resolve/main/vicuna-13b-v1.5.Q8_0.gguf

## karakuri-lm-8x7b-chat-v0.1-Q6_K.gguf
$ wget https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf/resolve/main/karakuri-lm-8x7b-chat-v0.1-Q6_K.gguf
```


<hr>


## Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ä½œæˆ
[abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)ã®GitHubã«ã‚ã‚‹ã€[Dockerfile](https://github.com/abetlen/llama-cpp-python/blob/main/docker/cuda_simple/Dockerfile)ã‹ã‚‰å®Ÿè¡Œç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ `llamacpp`ã‚’ä½œæˆã™ã‚‹ã€‚
```bash
$ cd $HOME/llamacpp ## llamma-cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
$ git clone https://github.com/abetlen/llama-cpp-python.git
$ cd llama-cpp-python/docker/cuda_simple/ ## CUDAæ¿Dockerimage
$ docker build -t llamacpp . ## ãƒ“ãƒ«ãƒ‰
```


### Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ç¢ºèª
```bash
$ docker images	
 REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
llamacpp     latest    f7da52f907e2   9 seconds ago   8.21GB
```


<hr>


## llama-cpp-pythonã‚µãƒ¼ãƒã®èµ·å‹•
### configãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚  
`$HOME/llamacpp/config.json`  
```json
{
  "host": "0.0.0.0",
  "port": 20080,
  "models": [
    {
      "model": "/llamacpp/model/vicuna-13b-v1.5.Q8_0.gguf",
      "model_alias": "vicuna-13b",
      "chat_format": "vicuna",
      "n_gpu_layers": -1,
      "n_ctx": 4096
    },
    {
      "model": "/llamacpp/model/karakuri-lm-8x7b-chat-v0.1-Q6_K.gguf",
      "model_alias": "karakuri-8x7b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "n_ctx": 4096
    }
  ]
}
```

è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°ã¯æœ«å°¾ã®ã€Œå‚è€ƒã€ã‚’å‚ç…§ã€‚
- model  
    ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
- model_alias  
    OpenAI APIã®`model`ã§æŒ‡å®šã•ã‚Œã‚‹åå‰ã€‚`gpt-3.5-turbo`ã‚„`text-davinch-003`ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å‡ºæ¥ã‚‹ã€‚
- chat_format  
    ãƒ¢ãƒ‡ãƒ«ã®Prompt Templateã®æŒ‡å®šã€‚`llama_chat_format.py`ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
- n_gpu_layers  
    GPUãŒä½¿ç”¨ã™ã‚‹layersæ•°ã€‚`-1`ã«ã™ã‚‹ã¨å…¨ã¦å‰²ã‚Šå½“ã¦ã‚‹ã€‚  
    ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã—ã¦ã„ã‚‹layersæ•°ã¯ã€èµ·å‹•æ™‚ã«`llm_load_tensors: offloaded 41/41 layers to GPU`ã®å½¢ã§è¡¨ç¤ºã•ã‚Œã‚‹ã®ã§ç¢ºèªã§ãã‚‹ã€‚
- n_ctx  
    æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· (default: 2048)


### èµ·å‹•
llama_cpp.serverã¯ã€ä½¿ç”¨ã™ã‚‹GPUã‚’æŒ‡å®šã§ããªã„ã£ã½ã„ï¼ˆå…¨GPUã‚’ä½¿ã£ã¦ã—ã¾ã†ï¼‰ã®ã§ã€è¤‡æ•°GPUã‚’æ­è¼‰ã—ã¦ã„ã‚‹å ´åˆã€Dockerã®`--gpu`ã§ä½¿ç”¨ã™ã‚‹GPUã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã€‚
```bash
$ docker run --rm -p 20080:20080 --gpus device=0 --cap-add SYS_RESOURCE -e USE_MLOCK=0 -v /home/llm/llamacpp:/llamacpp -h llamacpp --name llamacpp llamacpp python3 -m llama_cpp.server --config_file /llamacpp/config.json
```

èµ·å‹•å¾Œã€ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã€‚ä»¥ä¸‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºãŸã‚‰èµ·å‹•å®Œäº†ã€‚  
`CTRL+C`ã§çµ‚äº†ã€Dockerã‚‚ã‚³ãƒ³ãƒ†ãƒŠã‚‚å‰Šé™¤ã•ã‚Œã‚‹(`--rm`ã‚ªãƒ—ã‚·ãƒ§ãƒ³)ã€‚
```bash
 :
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:20080 (Press CTRL+C to quit)
```


<hr>


## èµ·å‹•å¾Œã®ç¢ºèª
### Chat completionã®ç¢ºèª
curlã§`vicuna-13b`ã«Chat APIã§å•ã„åˆã‚ã›ã¦ã¿ã‚‹ã€‚
```bash
$ time curl http://localhost:20080/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer None" \
-d '{
  "model": "vicuna-13b",
  "templature": 0.9,
  "top_p": 1.0,
  "messages": [
    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªè¦³å…‰ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚"},
    {"role": "user", "content": "æ—¥æœ¬ã®éƒ½é“åºœçœŒåã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ï¼‘ã¤å›ç­”ã—ã¦ãã ã•ã„ã€‚ãã®éƒ½é“åºœçœŒåã®é­…åŠ›ã‚’3ã¤ç­”ãˆã¦ãã ã•ã„ã€‚"}
  ]
}' | jq
```

å›ç­”ä¾‹
> `jq`ã‚³ãƒãƒ³ãƒ‰ã‚’ä½µç”¨ã™ã‚‹ã¨ã€jsonãŒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚Œã¦å‡ºåŠ›ã•ã‚Œã‚‹ã€‚
```json
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1270  100   929  100   341    145     53  0:00:06  0:00:06 --:--:--   222
{
  "id": "chatcmpl-aef316e8-2b0e-4d2f-82a1-8baa3b62f808",
  "object": "chat.completion",
  "created": 1715497858,
  "model": "vicuna-13b",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": " ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã éƒ½é“åºœçœŒã¯ã€Œå²é˜œçœŒã€ã§ã™ã€‚\n\n1. å²é˜œçœŒã®é­…åŠ›ã®ä¸€ã¤ã¯ã€ç¾ã—ã„è‡ªç„¶ãŒæ®‹ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚çœŒå†…ã«ã¯å¤šãã®å±±ã‚„æ£®ãŒã‚ã‚Šã€ãã®ä¸­ã«ã¯æ—¥æœ¬ã®åæ°´ã€Œé•·è‰¯å·ã€ã‚‚æµã‚Œã¦ã„ã¾ã™ã€‚\n2. å²é˜œçœŒã¯é£Ÿæ–‡åŒ–ã‚‚æœ‰åã§ã™ã€‚ä»£è¡¨çš„ãªã‚‚ã®ã¨ã—ã¦ã€ŒãŠã ã‚ã‚Šã€ã‚„ã€Œã‹ã¤ãŠç¯€ã€ãŒã‚ã‚Šã€å¤šãã®è¦³å…‰å®¢ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚\n3. å²é˜œçœŒã¯æ­´å²ã«ã‚‚ã‚ãµã‚Œã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€ç¾æ¿ƒå›½åˆ†å¯ºã‚„é•·è‰¯å·é‰„é“æ²¿ç·šã®å°ã•ãªç”ºä¸¦ã¿ãªã©ã€æ­´å²ã‚’æ„Ÿã˜ã‚‰ã‚Œã‚‹å ´æ‰€ãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™ã€‚",
        "role": "assistant"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 92,
    "completion_tokens": 253,
    "total_tokens": 345
  }
}

real	0m6.383s
user	0m0.024s
sys	0m0.001s
```


### ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰ãˆã¦å•ã„åˆã‚ã›ã‚‹
curlã§`"model": "karakuri-8x7b"`ã«å¤‰ãˆã‚‹ã¨ã€å‡¦ç†ã‚’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ›´ã•ã‚Œã‚‹ã€‚ã“ã®æ™‚ã€Dockerå´ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¥ã‚Œæ›¿ãˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€‚  
åˆå›ã®èª­ã¿è¾¼ã¿ã¯ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŒã€2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã®ã§ã€ã‹ãªã‚Šé«˜é€Ÿã§ã§åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã€‚
```bash
$ time curl http://localhost:20080/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer None" \
-d '{
  "model": "karakuri-8x7b",
  "templature": 0.9,
  "top_p": 1.0,
  "messages": [
    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªè¦³å…‰ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚"},
    {"role": "user", "content": "æ—¥æœ¬ã®éƒ½é“åºœçœŒåã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ï¼‘ã¤å›ç­”ã—ã¦ãã ã•ã„ã€‚ãã®éƒ½é“åºœçœŒåã®é­…åŠ›ã‚’3ã¤ç­”ãˆã¦ãã ã•ã„ã€‚"}
  ]
}' | jq
```


### Embeddings
curlã§`vicuna-13b`ã«Embeddingsã‚’å•ã„åˆã‚ã›ã¦ã¿ã‚‹ã€‚
```bash
time curl http://localhost:20080/v1/embeddings \
-H "Content-Type: application/json" \
-H "Authorization: Bearer None" \
-d '{
  "model": "vicuna-13b",
  "input": "query: å¤•é£¯ã¯ãŠè‚‰ã§ã™ã€‚"
}' | jq |less
```
å›ç­”ä¾‹
```bash
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [
        [
          0.1092359870672226,
                 :
                çœç•¥
                 :
          -0.8064247965812683
        ]
      ],
      "index": 0
    }
  ],
  "model": "vicuna-13b",
  "usage": {
    "prompt_tokens": 18,
    "total_tokens": 18
  }
}
```


<br>
<hr>

## å‚è€ƒ

### chat_format
`--chat_fomat`ã§æŒ‡å®šå¯èƒ½ãªå€¤ã¯ã€llama_chat_format.pyã‚’@register_chat_formatã§grepã—ã¦ç¢ºèªã§ãã‚‹ã€‚  
```
$ grep @register_chat_format ~/llamacpp/llama-cpp-python/llama_cpp/llama_chat_format.py
---
@register_chat_format("llama-2")
@register_chat_format("llama-3")
@register_chat_format("alpaca")
@register_chat_format("qwen")
@register_chat_format("vicuna")
@register_chat_format("oasst_llama")
@register_chat_format("baichuan-2")
@register_chat_format("baichuan")
@register_chat_format("openbuddy")
@register_chat_format("redpajama-incite")
@register_chat_format("snoozy")
@register_chat_format("phind")
@register_chat_format("intel")
@register_chat_format("open-orca")
@register_chat_format("mistrallite")
@register_chat_format("zephyr")
@register_chat_format("pygmalion")
@register_chat_format("chatml")
@register_chat_format("mistral-instruct")
@register_chat_format("chatglm3")
@register_chat_format("openchat")
@register_chat_format("saiga")
@register_chat_format("gemma")
````

### èµ·å‹•ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```
usage: __main__.py [-h] [--model MODEL] [--model_alias MODEL_ALIAS] [--n_gpu_layers N_GPU_LAYERS]
                   [--split_mode SPLIT_MODE] [--main_gpu MAIN_GPU]
                   [--tensor_split [TENSOR_SPLIT ...]] [--vocab_only VOCAB_ONLY]
                   [--use_mmap USE_MMAP] [--use_mlock USE_MLOCK]
                   [--kv_overrides [KV_OVERRIDES ...]] [--seed SEED] [--n_ctx N_CTX]
                   [--n_batch N_BATCH] [--n_threads N_THREADS] [--n_threads_batch N_THREADS_BATCH]
                   [--rope_scaling_type ROPE_SCALING_TYPE] [--rope_freq_base ROPE_FREQ_BASE]
                   [--rope_freq_scale ROPE_FREQ_SCALE] [--yarn_ext_factor YARN_EXT_FACTOR]
                   [--yarn_attn_factor YARN_ATTN_FACTOR] [--yarn_beta_fast YARN_BETA_FAST]
                   [--yarn_beta_slow YARN_BETA_SLOW] [--yarn_orig_ctx YARN_ORIG_CTX]
                   [--mul_mat_q MUL_MAT_Q] [--logits_all LOGITS_ALL] [--embedding EMBEDDING]
                   [--offload_kqv OFFLOAD_KQV] [--flash_attn FLASH_ATTN]
                   [--last_n_tokens_size LAST_N_TOKENS_SIZE] [--lora_base LORA_BASE]
                   [--lora_path LORA_PATH] [--numa NUMA] [--chat_format CHAT_FORMAT]
                   [--clip_model_path CLIP_MODEL_PATH] [--cache CACHE] [--cache_type CACHE_TYPE]
                   [--cache_size CACHE_SIZE] [--hf_tokenizer_config_path HF_TOKENIZER_CONFIG_PATH]
                   [--hf_pretrained_model_name_or_path HF_PRETRAINED_MODEL_NAME_OR_PATH]
                   [--hf_model_repo_id HF_MODEL_REPO_ID] [--draft_model DRAFT_MODEL]
                   [--draft_model_num_pred_tokens DRAFT_MODEL_NUM_PRED_TOKENS] [--type_k TYPE_K]
                   [--type_v TYPE_V] [--verbose VERBOSE] [--host HOST] [--port PORT]
                   [--ssl_keyfile SSL_KEYFILE] [--ssl_certfile SSL_CERTFILE] [--api_key API_KEY]
                   [--interrupt_requests INTERRUPT_REQUESTS]
                   [--disable_ping_events DISABLE_PING_EVENTS] [--root_path ROOT_PATH]
                   [--config_file CONFIG_FILE]

ğŸ¦™ Llama.cpp python server. Host your own LLMs!ğŸš€

options:
  -h, --help            show this help message and exit
  --model MODEL         The path to the model to use for generating completions.
  --model_alias MODEL_ALIAS
                        The alias of the model to use for generating completions.
  --n_gpu_layers N_GPU_LAYERS
                        The number of layers to put on the GPU. The rest will be on the CPU. Set
                        -1 to move all to GPU.
  --split_mode SPLIT_MODE
                        The split mode to use. (default: 1)
  --main_gpu MAIN_GPU   Main GPU to use.
  --tensor_split [TENSOR_SPLIT ...]
                        Split layers across multiple GPUs in proportion.
  --vocab_only VOCAB_ONLY
                        Whether to only return the vocabulary.
  --use_mmap USE_MMAP   Use mmap. (default: True)
  --use_mlock USE_MLOCK
                        Use mlock. (default: True)
  --kv_overrides [KV_OVERRIDES ...]
                        List of model kv overrides in the format key=type:value where type is one
                        of (bool, int, float). Valid true values are (true, TRUE, 1), otherwise
                        false.
  --seed SEED           Random seed. -1 for random. (default: 4294967295)
  --n_ctx N_CTX         The context size. (default: 2048)
  --n_batch N_BATCH     The batch size to use per eval. (default: 512)
  --n_threads N_THREADS
                        The number of threads to use. Use -1 for max cpu threads (default: 24)
  --n_threads_batch N_THREADS_BATCH
                        The number of threads to use when batch processing. Use -1 for max cpu
                        threads (default: 48)
  --rope_scaling_type ROPE_SCALING_TYPE
  --rope_freq_base ROPE_FREQ_BASE
                        RoPE base frequency
  --rope_freq_scale ROPE_FREQ_SCALE
                        RoPE frequency scaling factor
  --yarn_ext_factor YARN_EXT_FACTOR
  --yarn_attn_factor YARN_ATTN_FACTOR
  --yarn_beta_fast YARN_BETA_FAST
  --yarn_beta_slow YARN_BETA_SLOW
  --yarn_orig_ctx YARN_ORIG_CTX
  --mul_mat_q MUL_MAT_Q
                        if true, use experimental mul_mat_q kernels (default: True)
  --logits_all LOGITS_ALL
                        Whether to return logits. (default: True)
  --embedding EMBEDDING
                        Whether to use embeddings. (default: True)
  --offload_kqv OFFLOAD_KQV
                        Whether to offload kqv to the GPU. (default: True)
  --flash_attn FLASH_ATTN
                        Whether to use flash attention.
  --last_n_tokens_size LAST_N_TOKENS_SIZE
                        Last n tokens to keep for repeat penalty calculation. (default: 64)
  --lora_base LORA_BASE
                        Optional path to base model, useful if using a quantized base model and
                        you want to apply LoRA to an f16 model.
  --lora_path LORA_PATH
                        Path to a LoRA file to apply to the model.
  --numa NUMA           Enable NUMA support.
  --chat_format CHAT_FORMAT
                        Chat format to use.
  --clip_model_path CLIP_MODEL_PATH
                        Path to a CLIP model to use for multi-modal chat completion.
  --cache CACHE         Use a cache to reduce processing times for evaluated prompts.
  --cache_type CACHE_TYPE
                        The type of cache to use. Only used if cache is True. (default: ram)
  --cache_size CACHE_SIZE
                        The size of the cache in bytes. Only used if cache is True. (default:
                        2147483648)
  --hf_tokenizer_config_path HF_TOKENIZER_CONFIG_PATH
                        The path to a HuggingFace tokenizer_config.json file.
  --hf_pretrained_model_name_or_path HF_PRETRAINED_MODEL_NAME_OR_PATH
                        The model name or path to a pretrained HuggingFace tokenizer model. Same
                        as you would pass to AutoTokenizer.from_pretrained().
  --hf_model_repo_id HF_MODEL_REPO_ID
                        The model repo id to use for the HuggingFace tokenizer model.
  --draft_model DRAFT_MODEL
                        Method to use for speculative decoding. One of (prompt-lookup-decoding).
  --draft_model_num_pred_tokens DRAFT_MODEL_NUM_PRED_TOKENS
                        Number of tokens to predict using the draft model. (default: 10)
  --type_k TYPE_K       Type of the key cache quantization.
  --type_v TYPE_V       Type of the value cache quantization.
  --verbose VERBOSE     Whether to print debug information. (default: True)
  --host HOST           Listen address (default: localhost)
  --port PORT           Listen port (default: 8000)
  --ssl_keyfile SSL_KEYFILE
                        SSL key file for HTTPS
  --ssl_certfile SSL_CERTFILE
                        SSL certificate file for HTTPS
  --api_key API_KEY     API key for authentication. If set all requests need to be authenticated.
  --interrupt_requests INTERRUPT_REQUESTS
                        Whether to interrupt requests when a new request is received. (default:
                        True)
  --disable_ping_events DISABLE_PING_EVENTS
                        Disable EventSource pings (may be needed for some clients).
  --root_path ROOT_PATH
                        The root path for the server. Useful when running behind a reverse proxy.
  --config_file CONFIG_FILE
                        Path to a config file to load.
```

### ãƒªãƒ³ã‚¯
- [OpenAI Compatible Server](https://llama-cpp-python.readthedocs.io/en/latest/server/)  


<hr>

LLMå®Ÿè¡Œå§”å“¡ä¼š