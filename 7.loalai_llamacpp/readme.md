# LLMå®Ÿè¡Œç’°å¢ƒã®æ§‹ç¯‰
# LocalAI + llama-cpp

LocalAIçµŒç”±ã§llama-cppã‚’èµ·å‹•ã—ã¦ã€GGUFã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®æ‰‹é †ã€‚llama-cppã®Continuous Batchã«ã‚ˆã‚‹åŒæ™‚å‡¦ç†ãŒå¯èƒ½ã€‚


## æ§‹æˆæƒ…å ±
### å¤‰æ›ãƒ¢ãƒ‡ãƒ«æƒ…å ±

ã‚ã‚‰ã‹ã˜ã‚ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’`$HOME/vllm/model`ã«é…ç½®ã—ã¦ãŠãã€‚
- vicuna-13b-v1.5.Q8_0.gguf  
- karakuri-lm-8x7b-chat-v0.1-Q6_K.gguf
- karakuri-lm-70b-chat-v0.1-q4_K_M.gguf

### Dockerè¨­å®š

Docker `vllm`ã¯ä»¥ä¸‹ã®è¨­å®šã‚’ä½¿ç”¨ã™ã‚‹ã€‚

|Dockerå|Host Port|Docker Port|Host Dir|Docker Dir|
|:----|:----|:----|:----|:----|
|vllm|30080|30080|/home/llm/vllm|/vllm|


<br>
<hr>


# æ§‹ç¯‰æ‰‹é †
## Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ä½œæˆ
[mudler/localai](https://github.com/mudler/LocalAI)ã®GitHubã«ã‚ã‚‹ã€[Dockerfile](https://github.com/mudler/LocalAI/blob/master/Dockerfile)ã‹ã‚‰å®Ÿè¡Œç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ `localai`ã‚’ä½œæˆã™ã‚‹ã€‚

```bash
$ cd $HOME/localai 
$ git clone https://github.com/mudler/LocalAI.git
$ cd LocalAI
$ vi .env
```

llama-cppãŒGPUã‚’ä½¿ç”¨ã—ã€ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã«ãªã‚‹ã‚ˆã†ã«`.env`ã‚’å¤‰æ›´ã—ãƒ“ãƒ«ãƒ‰ã™ã‚‹ã€‚  å„é …ç›®ã®æ¦‚è¦ã¯`.env`ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã€‚

`$HOME/LocalAI/.env`
```bash
LOCALAI_THREADS=8
LOCALAI_LOG_LEVEL=debug
LOCALAI_SINGLE_ACTIVE_BACKEND=true
BUILD_TYPE=cublas
PYTHON_GRPC_MAX_WORKERS=8
LLAMACPP_PARALLEL=8
LOCALAI_PARALLEL_REQUESTS=true
```
- BUILD_TYPE: GPU(cublas)ã‚’ä½¿ã†
- LOCALAI_SINGLE_ACTIVE_BACKEND: trueã ã¨è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«èµ·å‹•ã—ãªã„ï¼ˆéƒ½åº¦å…¥ã‚Œæ›¿ãˆã‚‹ï¼‰
- LOCALAI_PARALLEL_REQUESTS: Continuous Batchå¯èƒ½


### Dockerãƒ“ãƒ«ãƒ‰
`--build-arg`ã§è¨­å®šã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã€‚
```
$ docker build --build-arg BUILD_TYPE=cublas -t localai .
```

<br>
<hr>


## localaiã‚µãƒ¼ãƒã®è¨­å®š
yamlã®è¨­å®šé …ç›®ã¯ã€[Advanced configuration with YAML files](https://localai.io/advanced/)ã‚’å‚ç…§ã€‚ãŸã ã—ã€ã‚ã¾ã‚Šè©³ã—ãã¯æ›¸ã„ã¦ã„ãªã„ãŸã‚ã€Prompt Templateç­‰ã¯ã€æœ«å°¾ã®ãƒªãƒ³ã‚¯ã‚’å‚è€ƒã«æƒ³åƒã™ã‚‹ã—ã‹ç„¡ã„ğŸ’¦

ä»¥ä¸‹ã¯ã€vicuna-13bã€karakuri-8x7bã€karakuri-70bã®è¨­å®šã€‚  
`$HOME/config.yaml`
```yaml
- name: vicuna-13b
  backend: llama-cpp
  embeddings: true
  context_size: 4095
  f16: true
  gpu_layers: 41
  mmap: true
  parameters:
    model: vicuna-13b-v1.5.Q8_0.gguf
    temperature: 0.7
    top_k: 40
    top_p: 0.7
    seed: -1
  template:
    chat: |
      A chat between a human and an assistant.

      ### Human: 
      {{.Input }}
      ### Assistant: 

- name: karakuri-8x7b
  backend: llama-cpp
  embeddings: true
  context_size: 4095
  f16: true
  gpu_layers: 33
  mmap: true
  mmlock: true
  batch: 512
  parameters:
    model: karakuri-lm-8x7b-chat-v0.1-Q6_K.gguf
    temperature: 0.7
    top_k: 40
    top_p: 0.7
    seed: -1
  template:
    chat: -
      [INST] {{.Input}} [/INST]
    completion: -
      [INST] {{.Input}} [/INST]

- name: karakuri-70b
  backend: llama-cpp
  embeddings: true
  context_size: 4095
  f16: true
  gpu_layers: 81
  mmap: true
  mmlock: true
  batch: 1024
  parameters:
    model: karakuri-lm-70b-chat-v0.1-q4_K_M.gguf
    temperature: 0.7
    top_k: 40
    top_p: 0.7
    seed: -1
  template:
    chat: |
      {{if eq .RoleName "assistant"}}{{.Content}}{{else}}
      [INST]
      {{if .SystemPrompt}}{{.SystemPrompt}}{{else if eq .RoleName "system"}}<<SYS>>{{.Content}}<</SYS>>

      {{else if .Content}}{{.Content}}{{end}}
      [/INST]
      {{end}}
```

### LocalAIã‚µãƒ¼ãƒã®èµ·å‹•
`-e`ã§è¨­å®šã‚’å¤‰ãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹ã€‚  
`LOCALAI_SINGLE_ACTIVE_BACKEND=true`ã ã¨ã€GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€å®Ÿè¡Œæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã€‚`false`ã«ã™ã‚‹ã¨ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒåŒæ™‚ã«èµ·å‹•ã•ã‚Œã‚‹ã€‚  
ãƒ­ã‚°ã®è¡¨ç¤ºã‚’æŠ‘ãˆãŸã„å ´åˆã€`LOCALAI_LOG_LEVEL=info`ã«ã™ã‚‹ã€‚
```bash
docker run --rm -p 40080:40080 --gpus all -v /home/llm/localai:/localai \
-e LOCALAI_LOG_LEVEL=debug \
-e LOCALAI_SINGLE_ACTIVE_BACKEND=true \
-e LOCALAI_PARALLEL_REQUESTS=true \
-h localai --name localai \
localai run  \
--config-file /localai/config.yaml \
--models-path /localai//model \
--address="0.0.0.0:40080"
```

<hr>

## å®Ÿè¡Œã®ç¢ºèª
curlã§modelåã‚’`vicuna-13b`ã€`karakuri-8x7b`ã€`karakuri-70b`ã«å¤‰ãˆã¦å®Ÿè¡Œã—ã¦ã¿ã‚‹ã€‚ã“ã®æ™‚ã‚µãƒ¼ãƒå´ã§ã¯ãƒ¢ãƒ‡ãƒ«ã®å…¥ã‚Œæ›¿ãˆãŒèµ·ãã‚‹ã€‚    
ã¾ãŸã€åŒã˜ãƒ¢ãƒ‡ãƒ«ã«è¤‡æ•°åŒæ™‚å•ã„åˆã‚ã›ã‚’è¡Œã£ã¦ã€ã»ã¼åŒæ™‚ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã‚‹ã‹ç¢ºèªã™ã‚‹(Continuous Batchã«ã‚ˆã‚‹åŒæ™‚å‡¦ç†)ã€‚  
é•ã†ãƒ¢ãƒ‡ãƒ«ã«åŒæ™‚ã«å•ã„åˆã‚ã›ãŸå ´åˆã€ã©ã¡ã‚‰ã‹ãŒ`{"error":{"code":500,"message":"could not load model: rpc error: code = Canceled desc = ","type":""}}`ã®ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã€‚
```bash
$ time curl http://localhost:40080/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer None" \
-d '{
  "model": "vicuna-13b",
  "templature": 0.1,
  "top_p": 0.1,
  "messages": [
    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªè¦³å…‰ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚"},
    {"role": "user", "content": "æ—¥æœ¬ã®éƒ½é“åºœçœŒåã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ï¼‘ã¤å›ç­”ã—ã¦ãã ã•ã„ã€‚ãã®éƒ½é“åºœçœŒåã®é­…åŠ›ã‚’3ã¤ç­”ãˆã¦ãã ã•ã„ã€‚"}
  ]
}'
```


<hr>

## èª²é¡Œ
LocalAIã‚’ä½¿ã†ã“ã¨ã«ã‚ˆã‚Šã€llama-cppã‚’ä½¿ã£ãŸGGUFå½¢å¼ã®Continuous Batchã«ã‚ˆã‚‹åŒæ™‚å‡¦ç†ã‚„è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨ãŒOpenAI APIå½¢å¼ã§åˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã€‚  
ã—ã‹ã—ã€å›ç­”ãŒé€”ä¸­ã§åˆ‡ã‚ŒãŸã‚Šã€æœ€å¾Œã«ç‰¹æ®Šè¨˜å·ãŒå…¥ã£ã¦ã—ã¾ã†å•é¡ŒãŒå¤šç™ºã™ã‚‹ãŸã‚ã€å®Ÿç”¨æ€§ã¯å³ã—ã„ã€‚ã“ã‚Œã‚‰ã¯llama-cpp-pythonã®ã‚µãƒ¼ãƒã§ã¯ç™ºç”Ÿã—ãªã„ãŸã‚ã€ä½•ã‚‰ã‹ã®åŸå› ãŒã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ãŒã€è³‡æ–™ãŒå°‘ãªã„ãŸã‚åŸå› ã®è§£æ˜ã«ã¯è‡³ã£ã¦ã„ãªã„ã€‚ã‚‚ã†å°‘ã—ã“ãªã‚Œã‚‹ã¾ã§ã€å¾…ã¤ã—ã‹ãªã„ã‹ã€‚ã€‚ã€‚



<br>
<hr>


## å‚è€ƒ
### ãƒªãƒ³ã‚¯
- [Metaã®ã€ŒLlama 3ã€ã‚’OpenAI APIäº’æ›ã®ã‚µãƒ¼ãƒãƒ¼ã‚’æŒã¤llama-cpp-pythonã¨LocalAIã§è©¦ã™](https://kazuhira-r.hatenablog.com/entry/2024/04/26/001435)  
- [Advanced configuration with YAML](https://localai.io/advanced/)  
- [BreadcrumbsLocalAI/embedded/models/](https://localai.io/advanced/)  
- [LocalAI/examples/configurations/](https://github.com/mudler/LocalAI/tree/master/examples/configurations)  
- [model-gallery/llama2-7b-chat-gguf.yaml](https://github.com/go-skynet/model-gallery/blob/main/llama2-7b-chat-gguf.yaml)  
- [prompt-templates/llama2-chat-message.tmpl](https://github.com/mudler/LocalAI/blob/master/prompt-templates/llama2-chat-message.tmpl)  
- [prompt-templates/vicuna.tmpl](https://github.com/mudler/LocalAI/blob/master/prompt-templates/vicuna.tmpl)  



<hr>

LLMå®Ÿè¡Œå§”å“¡ä¼š