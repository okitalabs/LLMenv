# LLMå®Ÿè¡Œç’°å¢ƒã®æ§‹ç¯‰
# GPTQå¤‰æ›

Vicuna-13b-1.5ã®16bitãƒ¢ãƒ‡ãƒ«ã‚’GPTQ 8bitã«å¤‰æ›ã™ã‚‹æ‰‹é †ã€‚  HuggingFaceã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹GPTQã®ã»ã¨ã‚“ã©ãŒ4bitã®ãŸã‚ã€8bitã§åˆ©ç”¨ã—ãŸã„å ´åˆã€è‡ªå‰ã§å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚  

ã“ã“ã§ã¯ã€vLLMã®Docker Imageã‚’åˆ©ç”¨ã—ã€GPTQå¤‰æ›ã«å¿…è¦ãªç’°å¢ƒã‚’æ§‹ç¯‰ã—vLLMã§å®Ÿè¡Œã™ã‚‹ã€‚



## æ§‹æˆæƒ…å ±
### å¤‰æ›ãƒ¢ãƒ‡ãƒ«æƒ…å ±
|modelå|ãƒ©ãƒ³ã‚¿ã‚¤ãƒ |å¤‰æ›å¾Œãƒ¢ãƒ‡ãƒ«å|
|:----|:----|:----|
|[lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)|GPTQ 8bit|vicuna-13b-v1.5-gptq_8bit|


### Dockerè¨­å®š
|Dockerå|Host Port|Docker Port|Host Dir|Docker Dir|
|:----|:----|:----|:----|:----|
|vllm_dev|30080|30080|/home/llm/vllm|/vllm|
||8888|38888|Juptter Port|


<br>
<hr>


# æ§‹ç¯‰æ‰‹é †
## LLMãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ã‚ã‚‰ã‹ã˜ã‚ã€å¤‰æ›å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãã€‚

### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
`dl_vicuna-13b.py`
```python
from huggingface_hub import snapshot_download, login
# login(token = "Toke ID") ## èªè¨¼ãŒå¿…è¦ãªå ´åˆ

model_name = "https://huggingface.co/lmsys/vicuna-13b-v1.5" ## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹HuggingFaceã®ãƒ¢ãƒ‡ãƒ«å
save_name = "/home/llm/vllm/model/vicuna-13b-v1.5" ## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

download_path = snapshot_download(
    repo_id = model_name,
    local_dir = save_name,
    local_dir_use_symlinks=False
)
```

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å®Ÿè¡Œ
```bash
$ # mkdir $HOME/vllm/model ## ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
$ cd $HOME/vllm/ ## ã“ã“ã«dl_vicuna-13b.pyã‚’é…ç½®
$ pip install huggingface_hub ## pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
$ python dl_vicuna-13b.py
```


<br>
<hr>

## vLLM GPTQã‚µãƒ¼ãƒã®èµ·å‹•
### Dockerèµ·å‹•
`--entrypoint ""`ã§vLLMã‚’èµ·å‹•ã›ãšã€`/bin/bash`ã‚’èµ·å‹•ã™ã‚‹ã€‚  
Port `3888:8888`ã¯Jupyterã‚’ä½¿ã†å ´åˆã€‚
```bash
$ docker run --rm -p 30080:30080 -p 38888:8888 --gpus all -it --entrypoint "" -v /home/llm/vllm:/vllm -h vllm_dev --name vllm_dev vllm /bin/bash
```
èµ·å‹•å¾Œã€Shellã«ãªã‚‹ã®ã§ã“ã“ã§ä½œæ¥­ã™ã‚‹ã€‚
```bash
root@vllm:/vllm-workspace#
```
ã•ã‚‰ã«ã‚³ãƒ³ãƒ†ãƒŠã«ãƒ­ã‚°ã‚¤ãƒ³ã—ãŸã„å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§Dockerã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹ã€‚
```
$ docker exec -it vllm_dev /bin/bash
root@vllm:/vllm-workspace#
```

### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
GPTQå¤‰æ›ã«å¿…è¦ã€‚
```
root@vllm:/vllm-workspace# cd /vllm ## ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¦ãŠã
root@vllm:/vllm# pip install auto-gptq 
```
Jupyterã‚’ä½¿ã†å ´åˆã«å¿…è¦ã€‚  
Jupyterã®èµ·å‹•å¾Œã€Hostã‹ã‚‰`http://localhost:38888`ã€Token`llm`ã§æ¥ç¶šã€‚
```
root@vllm:/vllm# pip install ipywidgets iprogress jupyterLab
root@vllm:/vllm# jupyter-lab --no-browser --port=8888 --ip=0.0.0.0 --allow-root --NotebookApp.token="llm"
```

### å¤‰æ›ã®å®Ÿè¡Œ
ä»¥ä¸‹ã®Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ã‚³ãƒãƒ³ãƒ‰ã¾ãŸã¯Jupyterã‹ã‚‰å®Ÿè¡Œã™ã‚‹ã€‚  

GPTQConfigã®`bits=`ã§é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•°ã‚’ã€  `dataset=`ã§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒ‡å®šã™ã‚‹ã€‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯'wikitext2','c4','c4-new','ptb','ptb-new'ã‹ã‚‰é¸ã¶ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ã€‚

```python
import torch, time
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig
stime = time.perf_counter() ## é–‹å§‹æ™‚é–“

model_name = "/vllm/model/vicuna-13b-v1.5" ## é‡å­åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
save_file = "/vllm/model/vicuna-13b-gptq-8bit" ## é‡å­åŒ–æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ
tokenizer = AutoTokenizer.from_pretrained(model_name)

## GPTQè¨­å®š
gptq_config = GPTQConfig(
bits=8, ## é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•°ã€Only support quantization to [2,3,4,8] bits
dataset='c4-new', ## ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ['wikitext2','c4','c4-new','ptb','ptb-new'] 'wikitext2','c4','c4-new','ptb','ptb-new'
tokenizer=tokenizer,
use_exllama=False,
cache_examples_on_gpu=False,
use_cuda_fp16=True
)

## GPUãƒ¡ãƒ¢ãƒªç¯€ç´„
my_device_map = {'model.embed_tokens': 'cpu', 'model.layers': 'cpu', 'model.norm': 'cpu', 'lm_head': 'cpu'}

## é‡å­åŒ–
## Auto-GPTQãŒGPTQConfigã®è¨­å®šã«å¾“ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã‚‹
model = AutoModelForCausalLM.from_pretrained(model_name, device_map=my_device_map,
torch_dtype=torch.float16, quantization_config=gptq_config)

## Vicunaã¯å…ƒã®generation_configã«ä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹
## ã“ã®è¨­å®šãŒã‚ã‚‹ã¨ä¿å­˜ã§ããªã„ã®ã§ã€Noneã«ã™ã‚‹ã‚‹
quantized_model.generation_config.temperature=None
quantized_model.generation_config.top_p=None
quantized_model.generation_config.max_length=None

## safetensorså½¢å¼ã§ä¿å­˜
quantized_model.to('cpu')
quantized_model.save_pretrained(save_file, safe_serialization=True)
tokenizer.save_pretrained(save_file)

print('time:', time.perf_counter() - stime) ##ã€€å‡¦ç†æ™‚é–“
```

> å‡¦ç†æ™‚é–“ã¯L40sã§20åˆ†ç¨‹åº¦ã€GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒªã¯44GBç¨‹åº¦å¿…è¦ã€‚  
> GPUãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªã„å ´åˆã€å¾Œè¿°ã®ã€ŒCPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹å¤‰æ›ã€ã‚’å‚ç…§ã€‚


### å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ã€‚
```
root@vllm:/vllm# ls /vllm/model/vicuna-13b-gptq-8bit/
config.json                       model-00003-of-00003.safetensors  tokenizer.model
generation_config.json            model.safetensors.index.json      tokenizer_config.json
model-00001-of-00003.safetensors  special_tokens_map.json
model-00002-of-00003.safetensors  tokenizer.json
```


<br>
<hr>


## vLLMã‚µãƒ¼ãƒã®èµ·å‹•
`vicuna-13b-gptq-8bit`ã‚’ãƒ¢ãƒ‡ãƒ«å`vicuna-13b`ã¨ã—ã¦vLLMã§èµ·å‹•ã—ã€å‹•ä½œã®ç¢ºèªã‚’ã™ã‚‹ã€‚


### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
vLLMã¯vicunaå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€åˆ¥é€”ã€jinjaå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šç¾©ã—ã¦æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

`/vllm/vicuna.jinja`
```jinja
{% for message in messages %}
    {% if (message['role'] == 'system') %}
        {{ 'A chat between a human and an assistant. ' + message['content'] }}
    {% elif (message['role'] == 'user') %}
        {{ '### Human: ' + message['content'] }}
    {% elif (message['role'] == 'assistant') %}
        {{ '### Assistant: ' + message['content'] }}
    {% endif %}
{% endfor %}
{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}
### Assistant:
{% endif %}
```

### vLLMã®èµ·å‹•
`--chat-template`ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã€‚
```
root@vllm:/vllm# python3 -m vllm.entrypoints.openai.api_server \
--model /vllm/model/vicuna-13b-gptq-8bit \
--served-model-name vicuna-13b \
--quantization gptq \
--max-model-len 4095 \
--gpu-memory-utilization 0.9 \
--host 0.0.0.0 \
--port 30080 \
--chat-template /vllm/vicuna.jinja
```

Hostã‹ã‚‰ç¢ºèªã€‚
```
$ time curl http://localhost:30080/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer None" \
-d '{
  "model": "vicuna-13b",
  "messages": [
    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªè¦³å…‰ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚"},
    {"role": "user", "content": "æ—¥æœ¬ã®éƒ½é“åºœçœŒåã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ï¼‘ã¤å›ç­”ã—ã¦ãã ã•ã„ã€‚ãã®éƒ½é“åºœçœŒåã®é­…åŠ›ã‚’3ã¤ç­”ãˆã¦ãã ã•ã„ã€‚"}
  ]
}'
```



<br>
<hr>

## CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹å¤‰æ›
GPUãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªã„å ´åˆã€CPUã®ãƒ¡ãƒ¢ãƒªã‚‚ä½¿ç”¨ã—ã¦å¤‰æ›ã™ã‚‹ã€‚

```python
import os, time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, AutoConfig
from accelerate import init_empty_weights, infer_auto_device_map

stime = time.perf_counter() ## è¨ˆæ¸¬é–‹å§‹

model_name = "/vllm/model/vicuna-13b-v1.5" ## é‡å­åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
save_dir = "/vllm/model/vicuna-13b-gptq-8bit-cpu" ## é‡å­åŒ–æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ


tokenizer = AutoTokenizer.from_pretrained(model_name)
config = AutoConfig.from_pretrained(model_name)

with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

my_device_map = infer_auto_device_map(
	model,
	max_memory={0:"16GiB", "cpu":"128GiB"}, ## GPU, CPUã®ãƒ¡ãƒ¢ãƒªé‡ã‚’æŒ‡å®š
	dtype=torch.float16
)

gptq_config = GPTQConfig(
    bits=8,	## é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•°ã€Only support quantization to [2,3,4,8] bits
    dataset="c4-new", ## ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ['wikitext2','c4','c4-new','ptb','ptb-new']
    tokenizer=tokenizer,
    use_exllama=False, 
    cache_examples_on_gpu=False, 
    use_cuda_fp16=True 
)

## Auto-GPTQãŒGPTQConfigã®è¨­å®šã«å¾“ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã‚‹
quantized_model = AutoModelForCausalLM.from_pretrained(
	model_name, 
	torch_dtype=torch.float16, 
	do_sample=True, 
	quantization_config=gptq_config
) 

## å¤‰æ›çµ‚äº†
print('conv Time:', time.perf_counter() - stime) ## è¨ˆæ¸¬çµ‚äº†

## Vicunaã¯å…ƒã®generation_configã«ä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹
## ã“ã®è¨­å®šãŒã‚ã‚‹ã¨ä¿å­˜ã§ããªã„ã®ã§ã€Noneã«ã™ã‚‹
quantized_model.generation_config.temperature=None
quantized_model.generation_config.top_p=None
quantized_model.generation_config.max_length=None

## quantized_modelã®ä¿å­˜
quantized_model.to('cpu') ## å…¨ã¦CPUãƒ¡ãƒ¢ãƒªã«ç§»ã™
quantized_model.save_pretrained(save_dir, safe_serialization=True) ## safetensorså½¢å¼ã§ä¿å­˜
tokenizer.save_pretrained(save_dir)

print('Total Time:', time.perf_counter() - stime) ## è¨ˆæ¸¬çµ‚äº†
```
å‡¦ç†æ™‚é–“ã¯1891ç§’ã€GPUãƒ¡ãƒ¢ãƒªã¯15982MiB

<br>
<hr>


## å‚è€ƒ
### ãƒªãƒ³ã‚¯
- [HugginFace Quantize ğŸ¤— Transformers models](https://huggingface.co/docs/transformers/ja/main_classes/quantization)
- [ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã€‘Hugging Faceã«ã‚ˆã‚‹GPTQé‡å­åŒ–ã‚¬ã‚¤ãƒ‰](https://note.com/bakushu/n/n6c560265b994)
- [NVIDIA RTX3060(12GB)ã§LLMã‚’è©¦ã™ï¼šGPTQé‡å­åŒ–](https://zenn.dev/to2watt/articles/e98cbb5c3231ab)
- [ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«ã‚‚ã£ã¨æ°—ã‚’é…ã‚ã†ã®è©±](https://note.com/sakusakumura/n/n7d7abca9b2e4)
- [transformers/utils/quantization_config.py](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py)
- [gpt-neox-20b ã‚’ 3090 x 2 ã§å‹•ã‹ã™ãƒ¡ãƒ¢(3090 x 1 ã§ã‚‚å‹•ã!)](https://qiita.com/syoyo/items/ba2c25a573ab8e338cd5)

<hr>

LLMå®Ÿè¡Œå§”å“¡ä¼š